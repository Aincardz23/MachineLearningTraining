{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image-classification-dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOkp6iPwsOWHdO4dxyY4ifD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **The Image Classification Dataset**"
      ],
      "metadata": {
        "id": "N9L9U4Nv_hux"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkCG5qfX_eoC"
      },
      "outputs": [],
      "source": [
        "!pip3 install d2l"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "from d2l import torch as d2l\n",
        "\n",
        "d2l.use_svg_display()"
      ],
      "metadata": {
        "id": "CW_KJNg0_nZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reading the Dataset**\n",
        "We can [**download and read the Fashion-MNIST dataset into memory via the build-in functions in the framework.**]"
      ],
      "metadata": {
        "id": "d3fhtU-e_o9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# `ToTensor` converts the image data from PIL type to 32-bit floating point\n",
        "# tensors. It divides all numbers by 255 so that all pixel values are between\n",
        "# 0 and 1\n",
        "trans = transforms.ToTensor()\n",
        "mnist_train = torchvision.datasets.FashionMNIST(\n",
        "    root=\"../data\", train=True, transform=trans, download=True)\n",
        "mnist_test = torchvision.datasets.FashionMNIST(\n",
        "    root=\"../data\", train=False, transform=trans, download=True)"
      ],
      "metadata": {
        "id": "uLRdvR39_twr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fashion-MNIST consists of images from 10 categories, each represented by 6000 images in the training dataset and by 1000 in the test dataset. A test dataset (or test set) is used for evaluating model performance and not for training. Consequently the training set and the test set contain 60000 and 10000 images, respectively."
      ],
      "metadata": {
        "id": "qwonox-g_z1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The height and width of each input image are both 28 pixels. Note that the dataset consists of grayscale images, whose number of channels is 1. For brevity, throughout this book we store the shape of any image with height  width  pixels as  or (, )."
      ],
      "metadata": {
        "id": "IxIMXP1j_6TK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train[0][0].shape"
      ],
      "metadata": {
        "id": "86n5AZ9A_43y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The images in Fashion-MNIST are associated with the following categories: t-shirt, trousers, pullover, dress, coat, sandal, shirt, sneaker, bag, and ankle boot. The following function converts between numeric label indices and their names in text."
      ],
      "metadata": {
        "id": "D4_nl5H5ACP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fashion_mnist_labels(labels):\n",
        "    \"\"\"Return text labels for the Fashion-MNIST dataset.\"\"\"\n",
        "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
        "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
        "    return [text_labels[int(i)] for i in labels]"
      ],
      "metadata": {
        "id": "Meqog3RDAC4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now create a function to visualize these examples."
      ],
      "metadata": {
        "id": "mZbQrM_fAEUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n",
        "    \"\"\"Plot a list of images.\"\"\"\n",
        "    figsize = (num_cols * scale, num_rows * scale)\n",
        "    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\n",
        "    axes = axes.flatten()\n",
        "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
        "        if torch.is_tensor(img):\n",
        "            # Tensor Image\n",
        "            ax.imshow(img.numpy())\n",
        "        else:\n",
        "            # PIL Image\n",
        "            ax.imshow(img)\n",
        "        ax.axes.get_xaxis().set_visible(False)\n",
        "        ax.axes.get_yaxis().set_visible(False)\n",
        "        if titles:\n",
        "            ax.set_title(titles[i])\n",
        "    return axes"
      ],
      "metadata": {
        "id": "uQtgF9P1AGun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are [**the images and their corresponding labels**] (in text) for the first few examples in the training dataset."
      ],
      "metadata": {
        "id": "YRHoSXUYAINo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))\n",
        "show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y));"
      ],
      "metadata": {
        "id": "OpMpXR8ZAKf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading a Minibatch\n",
        "To make our life easier when reading from the training and test sets, we use the built-in data iterator rather than creating one from scratch. Recall that at each iteration, a data iterator [**reads a minibatch of data with size batch_size each time.**] We also randomly shuffle the examples for the training data iterator."
      ],
      "metadata": {
        "id": "59ckvZP0ANsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "\n",
        "def get_dataloader_workers(): \n",
        "    \"\"\"Use 4 processes to read the data.\"\"\"\n",
        "    return 4\n",
        "\n",
        "train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
        "                             num_workers=get_dataloader_workers())"
      ],
      "metadata": {
        "id": "lv5A6BZoAPDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us look at the time it takes to read the training data"
      ],
      "metadata": {
        "id": "biAbX6LlAZ94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timer = d2l.Timer()\n",
        "for X, y in train_iter:\n",
        "    continue\n",
        "f'{timer.stop():.2f} sec'"
      ],
      "metadata": {
        "id": "LsAkBjCgAa0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Putting All Things Together**"
      ],
      "metadata": {
        "id": "P0MU7R3AAeBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_fashion_mnist(batch_size, resize=None): \n",
        "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
        "    trans = [transforms.ToTensor()]\n",
        "    if resize:\n",
        "        trans.insert(0, transforms.Resize(resize))\n",
        "    trans = transforms.Compose(trans)\n",
        "    mnist_train = torchvision.datasets.FashionMNIST(\n",
        "        root=\"../data\", train=True, transform=trans, download=True)\n",
        "    mnist_test = torchvision.datasets.FashionMNIST(\n",
        "        root=\"../data\", train=False, transform=trans, download=True)\n",
        "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
        "                            num_workers=get_dataloader_workers()),\n",
        "            data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
        "                            num_workers=get_dataloader_workers()))"
      ],
      "metadata": {
        "id": "46vWKWuAAhDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter, test_iter = load_data_fashion_mnist(32, resize=64)\n",
        "for X, y in train_iter:\n",
        "    print(X.shape, X.dtype, y.shape, y.dtype)\n",
        "    break"
      ],
      "metadata": {
        "id": "hXckafuCAikO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}