{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TwitterSentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1-W_8U8jEGbRKqy7CO90xXAeEKtzRxO-w",
      "authorship_tag": "ABX9TyO1TlQv3tBWKUYGwlVUJ+wU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORT REQUIRED LIBRARIES**"
      ],
      "metadata": {
        "id": "553EsPmxGMLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsp_lSYyJ2OA",
        "outputId": "7d1fd679-11ae-4913-d202-8ab7010649e7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMme8iYOGJ_x",
        "outputId": "94491ece-7aed-4cf3-dada-8df7f9eb9785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import re, nltk\n",
        "nltk.download('punkt')\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import auc\n",
        "from matplotlib import pyplot\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.style.use('dark_background')\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORT DATASETS AND PARSE INTO COLUMNS**"
      ],
      "metadata": {
        "id": "QNRYGAscGgfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = '/content/drive/MyDrive/Colab Notebooks/Tweets.csv'\n",
        "df = pd.read_csv(data)\n",
        "df.head()\n",
        "#menggunakan metode dataframe dari library pandas untuk membagi data menjadi dua dimensional\n",
        "#dari sini bisa dilihat Tweet.csv gagal untuk di load"
      ],
      "metadata": {
        "id": "ac105lxsGf6F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "731bfbae-af22-49ba-e2a6-1bd1b2e64ad7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-f4d19b646b7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/Tweets.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#penggunaan metode dataframe dari library pandas untuk membagi data menjadi dua dimensional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 9, saw 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns\n",
        "#print index data"
      ],
      "metadata": {
        "id": "B8zI7fN9GvbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Check for NaN Values**\n",
        "\n",
        "Pemakaian metode isnull untuk memeriksa data null, data null dimasukkan ke counter True dan data sisanya False, jumlah nomor di sebelah kanan menandakan jumlah data yang null"
      ],
      "metadata": {
        "id": "TRCKEhe1Gxtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "G7bjYZnzG8OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plot Corellation Data**\n",
        "\n",
        "menggunakan metode cor untuk menemukan korelasi antara semua data dan menampilkannya menggunakan heatmap 2 dimensional"
      ],
      "metadata": {
        "id": "wfulkbejG9vG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,10)) #penggunanan size map 10x10\n",
        "corr = df.corr()\n",
        "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool) , cmap=sns.diverging_palette(-100,0,as_cmap=True) , square = True)"
      ],
      "metadata": {
        "id": "MJsjHSA4HD9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Selection**\n",
        "pembuangan data kurang penting agar data lebih mudah diproses, menyisakan data data yang penting"
      ],
      "metadata": {
        "id": "ECiqja1YHGv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unused_data = df[['tweet_id', 'negativereason', 'negativereason_confidence', 'airline_sentiment_gold', 'negativereason_gold', 'tweet_coord', 'tweet_location', 'user_timezone', 'name', 'retweet_count', 'tweet_created']]\n",
        "#pemilihan data kurang penting yang dipilih dengan cara manual\n",
        "df = df.drop(columns=unused_data)\n",
        "#buang data tadi dari tabel yang akan digunakan\n",
        "df"
      ],
      "metadata": {
        "id": "IzrP8Mv8HKtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exploring and Analyzing Data**\n",
        "The more a word pops up, the bigger it is on the plot\n",
        "\n",
        "Positive Sentiment"
      ],
      "metadata": {
        "id": "VghTus_wHJhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,20)) # plotting for positive sentiment words\n",
        "plt.grid(False)\n",
        "wc = WordCloud(width = 1600 , height = 800 , max_words = 3000).generate(\" \".join(df[df.airline_sentiment == 'positive'].text)) #library wordcloud untuk visualisasi sentimen positif\n",
        "plt.imshow(wc , interpolation = 'bilinear') #penggunaan interpolasi bilinear untuk display wordcloud"
      ],
      "metadata": {
        "id": "SDwpUuMkHSh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative Sentiment"
      ],
      "metadata": {
        "id": "bySQAGTxHUXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,20)) #plotting for positive sentiment words\n",
        "plt.grid(False)\n",
        "wc = WordCloud(width = 1600 , height = 800 , max_words = 3000).generate(\" \".join(df[df.airline_sentiment == 'negative'].text)) #library wordcloud untuk visualisasi sentimen positif\n",
        "plt.imshow(wc , interpolation = 'bilinear') #penggunaan interpolasi bilinear untuk display wordcloud"
      ],
      "metadata": {
        "id": "ueCoQkiKHVjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style=\"darkgrid\")\n",
        "sns.countplot(x = \"airline_sentiment\" , data = df)\n",
        "#pemisahan 2 sentimen untuk mengetahui siapa yang menggunakan banyak frekuensi"
      ],
      "metadata": {
        "id": "hcPTikXmHXAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build Function to Clean the Text and Call the Model**"
      ],
      "metadata": {
        "id": "XtD4QyoFHYZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fungsi cleaning tweet, digunakan untuk penyamaan format tweet, ie perubahan font kapital menjadi font kecil, est\n",
        "def clean_the_tweet(text):\n",
        "  tokens= nltk.word_tokenize(re.sub(\"[^a-zA-Z]\", \" \",text))\n",
        "  tokens = [token.lower() for token in tokens]\n",
        "  return ' '.join(tokens[2:])\n",
        "\n",
        "#fungsi pemrosesan kata dengan cara pembuangan kata yang tidak banyak memiliki arti dalam bahasa inggris\n",
        "def text_process(msg):\n",
        "  nopunc =[char for char in msg if char not in string.punctuation]\n",
        "  nopunc=''.join(nopunc)\n",
        "  return ' '.join([word for word in nopunc.split() if word.lower() not in stopwords.words('english')])\n",
        " \n",
        " #fungsi pengecekan skor masing-masing kata\n",
        "def check_scores(clf, X_train, X_test, y_train, y_test):\n",
        "  model = clf.fit(X_train, y_train)\n",
        "  predicted_class = model.predict(X_test)\n",
        "  predicted_class_train = model.predict(X_train)\n",
        "  test_probs = model.predict_proba(X_test)\n",
        "  test_probs = test_probs[:, 1]\n",
        "  yhat = model.predict(X_test)\n",
        "  lr_precision, lr_recall, _ = precision_recall_curve(y_test, test_probs)\n",
        "  lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n",
        "\n",
        "\n",
        "  print('Train confusion matrix is: ',)\n",
        "  print(confusion_matrix(y_train, predicted_class_train))\n",
        "  print()\n",
        "\n",
        "  print('Test confusion matrix is: ')\n",
        "  print(confusion_matrix(y_test, predicted_class))\n",
        "  print()\n",
        "\n",
        "  print(classification_report(y_test,predicted_class)) \n",
        "  print() \n",
        "\n",
        "  train_accuracy = accuracy_score(y_train,predicted_class_train)\n",
        "  test_accuracy = accuracy_score(y_test,predicted_class)\n",
        "\n",
        "  print(\"Train accuracy score: \", train_accuracy)\n",
        "  print(\"Test accuracy score: \",test_accuracy )\n",
        "  print()\n",
        "  \n",
        "  train_auc = roc_auc_score(y_train, clf.predict_proba(X_train)[:,1])\n",
        "  test_auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])\n",
        "\n",
        "  print(\"Train ROC-AUC score: \", train_auc)\n",
        "  print(\"Test ROC-AUC score: \", test_auc)\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "\n",
        "  ax1.plot(lr_recall, lr_precision)\n",
        "  ax1.set(xlabel=\"Recall\", ylabel=\"Precision\")\n",
        "\n",
        "  plt.subplots_adjust(left=0.5,\n",
        "                    bottom=0.1, \n",
        "                    right=1.5, \n",
        "                    top=0.9, \n",
        "                    wspace=0.4, \n",
        "                    hspace=0.4)\n",
        "  print()\n",
        "  print('Are under Precision-Recall curve:', lr_f1)\n",
        "  \n",
        "  fpr, tpr, _ = roc_curve(y_test, test_probs)\n",
        "\n",
        "\n",
        "  ax2.plot(fpr, tpr)\n",
        "  ax2.set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
        "\n",
        "  print(\"Area under ROC-AUC:\", lr_auc)\n",
        "  return train_accuracy, test_accuracy, train_auc, test_auc\n",
        "\n",
        "\n",
        "\n",
        "def grid_search(model, parameters, X_train, Y_train):\n",
        "  #Grid dibuat\n",
        "  grid = GridSearchCV(estimator=model,\n",
        "                       param_grid = parameters,\n",
        "                       cv = 2, verbose=2, scoring='roc_auc')\n",
        "  #lengkapi Grid \n",
        "  grid.fit(X_train,Y_train)\n",
        "  print()\n",
        "  print()\n",
        "  #model terbaik akan menjadi Grid\n",
        "  optimal_model = grid.best_estimator_\n",
        "  print('Best parameters are: ')\n",
        "  print( grid.best_params_)\n",
        "\n",
        "  return optimal_model"
      ],
      "metadata": {
        "id": "y7WJgqhHHdVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Applying Aforementioned Functions**"
      ],
      "metadata": {
        "id": "nWJw8rG3HezR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['airline_sentiment']!='neutral']\n",
        "df['cleaned_tweet'] = df['text'].apply(clean_the_tweet)\n",
        "\n",
        "df.head()\n",
        "df['airline_sentiment'] = df['airline_sentiment'].apply(lambda x: 1 if x =='positive' else 0)\n",
        "df.head()\n",
        "#filterasi tweet positif yang sentimen"
      ],
      "metadata": {
        "id": "ycMn3VCGHi6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_tweet'] = df['cleaned_tweet'].apply(text_process)\n",
        "df.reset_index(drop=True, inplace = True)\n",
        "df.head()\n",
        "#penggunaan kata henti untuk filterasi sentimen yang tidak akan digunakan"
      ],
      "metadata": {
        "id": "QnN8_SaAHk5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['airline_sentiment'].unique()"
      ],
      "metadata": {
        "id": "4Z-mDmBVHmS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing using TF-IDF**"
      ],
      "metadata": {
        "id": "b09ZhamxHoI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(use_idf=True, lowercase=True)\n",
        "X_tf_idf= vectorizer.fit_transform(df.cleaned_tweet) #menggunakan tf-idf untuk mengecek kata kata yang relevan\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_tf_idf, df['airline_sentiment'], random_state=42) #menggunakan random state yang sama dengan training"
      ],
      "metadata": {
        "id": "veDIM_VjHphf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Artificial Neural Network**\n",
        "Penggunaan algoritma artificial neural network untuk memproses data yang sudah dibersihkan sebelumnya"
      ],
      "metadata": {
        "id": "tcu3ynl0Hv5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [df['cleaned_tweet'][i] for i in range( len(df))]\n",
        "\n",
        "voc_size=5000\n",
        "\n",
        "onehot_=[one_hot(words,voc_size)for words in corpus] \n",
        "\n",
        "max_sent_length=max([len(i) for i in corpus])\n",
        "\n",
        "embedded_docs=pad_sequences(onehot_,padding='pre',maxlen=max_sent_length)\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Dense(512, input_dim = max_sent_length, activation='sigmoid'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "X_final=np.array(embedded_docs)\n",
        "y_final=np.array(df['airline_sentiment'])\n",
        "X_final.shape, y_final.shape"
      ],
      "metadata": {
        "id": "_DBjIEIAHymm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_NN, X_test_NN, y_train_NN, y_test_NN = train_test_split(X_final, y_final, test_size=0.3, random_state=42)\n",
        "model.fit(X_train_NN, y_train_NN, validation_data = (X_test_NN, y_test_NN), epochs=10, batch_size=64)"
      ],
      "metadata": {
        "id": "Vv_CA-ctH0Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred_NN=np.round(model.predict(X_test_NN))\n",
        "y_train_pred_NN=np.round(model.predict(X_train_NN))\n",
        "y_test_pred_NN"
      ],
      "metadata": {
        "id": "KXPvx8MKH1nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc_NN = accuracy_score(y_test_NN, y_test_pred_NN)\n",
        "train_acc_NN = accuracy_score(y_train_NN, y_train_pred_NN)\n",
        "test_roc_NN = roc_auc_score(y_test_NN, y_test_pred_NN)\n",
        "train_roc_NN = roc_auc_score(y_train_NN, y_train_pred_NN)"
      ],
      "metadata": {
        "id": "y6NOA5fzH22U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM**\n",
        "Penggunaan algoritma Long Short Term Memory Network untuk memproses data yang sudah dibersihkan sebelumnya"
      ],
      "metadata": {
        "id": "kLaQL81QH4IE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [df['cleaned_tweet'][i] for i in range( len(df))]\n",
        "\n",
        "voc_size=5000\n",
        "\n",
        "onehot_=[one_hot(words,voc_size)for words in corpus] \n",
        "\n",
        "max_sent_length=max([len(i) for i in corpus])\n",
        "\n",
        "embedded_docs=pad_sequences(onehot_,padding='pre',maxlen=max_sent_length)\n",
        "    \n",
        "embedding_vector_features=64\n",
        "model=Sequential()\n",
        "model.add(Embedding(voc_size,embedding_vector_features,input_length=max_sent_length))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "X_final=np.array(embedded_docs)\n",
        "y_final=np.array(df['airline_sentiment'])\n",
        "X_final.shape,y_final.shape"
      ],
      "metadata": {
        "id": "5OGsVhYvH6AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_LSTM, X_test_LSTM, y_train_LSTM, y_test_LSTM = train_test_split(X_final, y_final, test_size=0.3, random_state=42)\n",
        "model.fit(X_train_LSTM, y_train_LSTM, validation_data = (X_test_LSTM, y_test_LSTM), epochs=10, batch_size=64)"
      ],
      "metadata": {
        "id": "bG3uYm4VH7qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred_LSTM=np.round(model.predict(X_test_LSTM))\n",
        "y_train_pred_LSTM=np.round(model.predict(X_train_LSTM))\n",
        "y_test_pred_LSTM"
      ],
      "metadata": {
        "id": "02h6UgXYH8p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc_LSTM = accuracy_score(y_test_LSTM, y_test_pred_LSTM)\n",
        "train_acc_LSTM = accuracy_score(y_train_LSTM, y_train_pred_LSTM)\n",
        "test_roc_LSTM = roc_auc_score(y_test_LSTM, y_test_pred_LSTM)\n",
        "train_roc_LSTM = roc_auc_score(y_train_LSTM, y_train_pred_LSTM)"
      ],
      "metadata": {
        "id": "m2RhbAzfH9rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multinomial Naive Bayes**\n",
        "Penggunaan algoritma Multinomial Naive Bayes untuk memproses data yang sudah dibersihkan sebelumnya"
      ],
      "metadata": {
        "id": "JVojHg1cH-46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m_naive_bayes = MultinomialNB()\n",
        "m_train_accuracy, m_test_accuracy, m_train_auc, m_test_auc = check_scores(m_naive_bayes ,x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "xBppduWxIAtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Forest**\n",
        "Penggunaan algoritma Multinomial Naive Bayes untuk memproses data yang sudah dibersihkan sebelumnya"
      ],
      "metadata": {
        "id": "eOQfiLVDIDYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(random_state=0)\n",
        "r_train_accuracy, r_test_accuracy, r_train_auc, r_test_auc= check_scores(rf, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "1kYFeVqRIFxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SVM**\n",
        "Penggunaan algoritma Support Vector Machine untuk memproses data yang sudah dibersihkan sebelumnya"
      ],
      "metadata": {
        "id": "IJU9JEc2IHb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVM = svm.SVC(probability=True)\n",
        "s_train_accuracy, s_test_accuracy, s_train_auc, s_test_auc = check_scores(SVM, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "bbBQ-wpoIKDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summary From Multiple Algorithms**\n",
        "Rangkuman dari algoritma-algoritma yang digunakan"
      ],
      "metadata": {
        "id": "RAeLD4WtIMtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [('Artificial Neural Network',train_acc_NN, test_acc_NN, train_roc_NN, test_roc_NN),\n",
        "        ('LSTM',train_acc_LSTM, test_acc_LSTM, train_roc_LSTM, test_roc_LSTM),\n",
        "        ('Multinomial Naive Bayes',m_train_accuracy, m_test_accuracy, m_train_auc, m_test_auc),\n",
        "        ('Random Forest', r_train_accuracy, r_test_accuracy, r_train_auc, r_test_auc),\n",
        "        ('SVM', s_train_accuracy, s_test_accuracy, s_train_auc, s_test_auc)]\n",
        "\n",
        "\n",
        "Scores_ = pd.DataFrame(data = data, columns=['Model Name','Train Accuracy', 'Test Accuracy', 'Train ROC', 'Test ROC'])\n",
        "Scores_.set_index('Model Name', inplace = True)\n",
        "\n",
        "Scores_"
      ],
      "metadata": {
        "id": "i6GeziJEIPE5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}