{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPKceycw5KkANYCu+Czb1Ue"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Required Module(s)**"
      ],
      "metadata": {
        "id": "e3cx-vxt7JDZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T82giWA830Bv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.svm import SVR\n",
        "from sklearn import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification"
      ],
      "metadata": {
        "id": "v1s_DRmC7P5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = [[0, 0], [1, 1]]\n",
        "y = [0, 1]\n",
        "clf = svm.SVC()\n",
        "clf.fit(X, y)"
      ],
      "metadata": {
        "id": "Hva6Wi5t7RQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.predict([[2., 2.]])"
      ],
      "metadata": {
        "id": "9yyfJUz-7TVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get support vectors\n",
        "clf.support_vectors_\n",
        "\n",
        "# get indices of support vectors\n",
        "clf.support_\n",
        "# get number of support vectors for each class\n",
        "clf.n_support_"
      ],
      "metadata": {
        "id": "WA071jy07VfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multiclass Classification**"
      ],
      "metadata": {
        "id": "A9_buEZ57X3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = [[0], [1], [2], [3]]\n",
        "Y = [0, 1, 2, 3]\n",
        "clf = svm.SVC(decision_function_shape='ovo')\n",
        "clf.fit(X, Y)"
      ],
      "metadata": {
        "id": "6q-ySRR67aHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dec = clf.decision_function([[1]])\n",
        "dec.shape[1] # 4 classes: 4*3/2 = 6"
      ],
      "metadata": {
        "id": "FmXVAsCY7e_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.decision_function_shape = \"ovr\"\n",
        "dec = clf.decision_function([[1]])\n",
        "dec.shape[1] # 4 cla"
      ],
      "metadata": {
        "id": "d_1e0eni7hZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lin_clf = svm.LinearSVC()\n",
        "lin_clf.fit(X, Y)"
      ],
      "metadata": {
        "id": "Ie2Hpffm7irH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dec = lin_clf.decision_function([[1]])\n",
        "dec.shape[1]"
      ],
      "metadata": {
        "id": "PhmspJSE7jvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Unbalaced Problem**"
      ],
      "metadata": {
        "id": "paeu6MA_7k0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "NcfBo-1V7mfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVM : Separating hyperlance for unbalanced classes**\n",
        "Find the optimal separator hyperplane using SVC for unbalanced classes.\n",
        "\n",
        "We first find the dividing plane with plain SVC and then plot (dotted line) the dividing hyperplane with autocorrect for unbalanced classes.\n",
        "\n",
        ".. currentmodule:: sklearn.linear_model"
      ],
      "metadata": {
        "id": "RVTL9Lvv7n68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we create two clusters of random points\n",
        "n_samples_1 = 1000\n",
        "n_samples_2 = 100\n",
        "centers = [[0.0, 0.0], [2.0, 2.0]]\n",
        "clusters_std = [1.5, 0.5]\n",
        "X, y = make_blobs(\n",
        "    n_samples=[n_samples_1, n_samples_2],\n",
        "    centers=centers,\n",
        "    cluster_std=clusters_std,\n",
        "    random_state=0,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "# fit the model and get the separating hyperplane\n",
        "clf = svm.SVC(kernel=\"linear\", C=1.0)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# fit the model and get the separating hyperplane using weighted classes\n",
        "wclf = svm.SVC(kernel=\"linear\", class_weight={1: 10})\n",
        "wclf.fit(X, y)\n",
        "\n",
        "# plot the samples\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors=\"k\")\n",
        "\n",
        "# plot the decision functions for both classifiers\n",
        "ax = plt.gca()\n",
        "xlim = ax.get_xlim()\n",
        "ylim = ax.get_ylim()\n",
        "\n",
        "# create grid to evaluate model\n",
        "xx = np.linspace(xlim[0], xlim[1], 30)\n",
        "yy = np.linspace(ylim[0], ylim[1], 30)\n",
        "YY, XX = np.meshgrid(yy, xx)\n",
        "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
        "\n",
        "# get the separating hyperplane\n",
        "Z = clf.decision_function(xy).reshape(XX.shape)\n",
        "\n",
        "# plot decision boundary and margins\n",
        "a = ax.contour(XX, YY, Z, colors=\"k\", levels=[0], alpha=0.5, linestyles=[\"-\"])\n",
        "\n",
        "# get the separating hyperplane for weighted classes\n",
        "Z = wclf.decision_function(xy).reshape(XX.shape)\n",
        "\n",
        "# plot decision boundary and margins for weighted classes\n",
        "b = ax.contour(XX, YY, Z, colors=\"r\", levels=[0], alpha=0.5, linestyles=[\"-\"])\n",
        "\n",
        "plt.legend(\n",
        "    [a.collections[0], b.collections[0]],\n",
        "    [\"non weighted\", \"weighted\"],\n",
        "    loc=\"upper right\",\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iqrRSONd7uka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SVM Weighted Sample**\n",
        "Plot the decision function of a weighted dataset, where the size of the points is proportional to the weight.\n",
        "\n",
        "The weighting of the sample changes the scale of the C parameter, which means that the classifier places more emphasis on getting these points right. The effect may often be subtle.\n",
        "\n",
        "To emphasize the effect here, we specifically weigh the outliers, making the decision boundary deformation very visible."
      ],
      "metadata": {
        "id": "PCDMclVr7yUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_decision_function(classifier, sample_weight, axis, title):\n",
        "    # plot the decision function\n",
        "    xx, yy = np.meshgrid(np.linspace(-4, 5, 500), np.linspace(-4, 5, 500))\n",
        "\n",
        "    Z = classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # plot the line, the points, and the nearest vectors to the plane\n",
        "    axis.contourf(xx, yy, Z, alpha=0.75, cmap=plt.cm.bone)\n",
        "    axis.scatter(\n",
        "        X[:, 0],\n",
        "        X[:, 1],\n",
        "        c=y,\n",
        "        s=100 * sample_weight,\n",
        "        alpha=0.9,\n",
        "        cmap=plt.cm.bone,\n",
        "        edgecolors=\"black\",\n",
        "    )\n",
        "\n",
        "    axis.axis(\"off\")\n",
        "    axis.set_title(title)\n",
        "\n",
        "\n",
        "# we create 20 points\n",
        "np.random.seed(0)\n",
        "X = np.r_[np.random.randn(10, 2) + [1, 1], np.random.randn(10, 2)]\n",
        "y = [1] * 10 + [-1] * 10\n",
        "sample_weight_last_ten = abs(np.random.randn(len(X)))\n",
        "sample_weight_constant = np.ones(len(X))\n",
        "# and bigger weights to some outliers\n",
        "sample_weight_last_ten[15:] *= 5\n",
        "sample_weight_last_ten[9] *= 15\n",
        "\n",
        "# for reference, first fit without sample weights\n",
        "\n",
        "# fit the model\n",
        "clf_weights = svm.SVC(gamma=1)\n",
        "clf_weights.fit(X, y, sample_weight=sample_weight_last_ten)\n",
        "\n",
        "clf_no_weights = svm.SVC(gamma=1)\n",
        "clf_no_weights.fit(X, y)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "plot_decision_function(\n",
        "    clf_no_weights, sample_weight_constant, axes[0], \"Constant weights\"\n",
        ")\n",
        "plot_decision_function(clf_weights, sample_weight_last_ten, axes[1], \"Modified weights\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3SzHiMBz72QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Support Vector Regression (SVR) using linear and non-linear kernels**\n",
        "example of 1D regression using linear, polynomial and RBF kernel"
      ],
      "metadata": {
        "id": "jKK8b6gA75pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################################################\n",
        "# Generate sample data\n",
        "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
        "y = np.sin(X).ravel()\n",
        "\n",
        "# #############################################################################\n",
        "# Add noise to targets\n",
        "y[::5] += 3 * (0.5 - np.random.rand(8))\n",
        "\n",
        "# #############################################################################\n",
        "# Fit regression model\n",
        "svr_rbf = SVR(kernel=\"rbf\", C=100, gamma=0.1, epsilon=0.1)\n",
        "svr_lin = SVR(kernel=\"linear\", C=100, gamma=\"auto\")\n",
        "svr_poly = SVR(kernel=\"poly\", C=100, gamma=\"auto\", degree=3, epsilon=0.1, coef0=1)\n",
        "\n",
        "# #############################################################################\n",
        "# Look at the results\n",
        "lw = 2\n",
        "\n",
        "svrs = [svr_rbf, svr_lin, svr_poly]\n",
        "kernel_label = [\"RBF\", \"Linear\", \"Polynomial\"]\n",
        "model_color = [\"m\", \"c\", \"g\"]\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10), sharey=True)\n",
        "for ix, svr in enumerate(svrs):\n",
        "    axes[ix].plot(\n",
        "        X,\n",
        "        svr.fit(X, y).predict(X),\n",
        "        color=model_color[ix],\n",
        "        lw=lw,\n",
        "        label=\"{} model\".format(kernel_label[ix]),\n",
        "    )\n",
        "    axes[ix].scatter(\n",
        "        X[svr.support_],\n",
        "        y[svr.support_],\n",
        "        facecolor=\"none\",\n",
        "        edgecolor=model_color[ix],\n",
        "        s=50,\n",
        "        label=\"{} support vectors\".format(kernel_label[ix]),\n",
        "    )\n",
        "    axes[ix].scatter(\n",
        "        X[np.setdiff1d(np.arange(len(X)), svr.support_)],\n",
        "        y[np.setdiff1d(np.arange(len(X)), svr.support_)],\n",
        "        facecolor=\"none\",\n",
        "        edgecolor=\"k\",\n",
        "        s=50,\n",
        "        label=\"other training data\",\n",
        "    )\n",
        "    axes[ix].legend(\n",
        "        loc=\"upper center\",\n",
        "        bbox_to_anchor=(0.5, 1.1),\n",
        "        ncol=1,\n",
        "        fancybox=True,\n",
        "        shadow=True,\n",
        "    )\n",
        "\n",
        "fig.text(0.5, 0.04, \"data\", ha=\"center\", va=\"center\")\n",
        "fig.text(0.06, 0.5, \"target\", ha=\"center\", va=\"center\", rotation=\"vertical\")\n",
        "fig.suptitle(\"Support Vector Regression\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WFFW4YLA790B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SVM : Custom Kernel**\n",
        "Simple use of Support Vector Machines to classify samples. This will plot the decision surface and supporting vectors."
      ],
      "metadata": {
        "id": "8owRKYih8AH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import some data to play with\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data[:, :2]  # we only take the first two features. We could\n",
        "# avoid this ugly slicing by using a two-dim dataset\n",
        "Y = wine.target\n",
        "\n",
        "\n",
        "def my_kernel(X, Y):\n",
        "    \"\"\"\n",
        "    We create a custom kernel:\n",
        "\n",
        "                 (2  0)\n",
        "    k(X, Y) = X  (    ) Y.T\n",
        "                 (0  1)\n",
        "    \"\"\"\n",
        "    M = np.array([[2, 0], [0, 1.0]])\n",
        "    return np.dot(np.dot(X, M), Y.T)\n",
        "\n",
        "\n",
        "h = 0.02  # step size in the mesh\n",
        "\n",
        "# we create an instance of SVM and fit out data.\n",
        "clf = svm.SVC(kernel=my_kernel)\n",
        "clf.fit(X, Y)\n",
        "\n",
        "# Plot the decision boundary. For that, we will assign a color to each\n",
        "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Put the result into a color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "# Plot also the training points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired, edgecolors=\"k\")\n",
        "plt.title(\"3-Class classification using Support Vector Machine with custom kernel\")\n",
        "plt.axis(\"tight\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mUquBktA8D9X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}